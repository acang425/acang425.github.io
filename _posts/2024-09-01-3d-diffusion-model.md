---
title: 论文阅读笔记：3d扩散模型
date: 2024-08-31 18:36:00 +/-0800
categories: [Deep Learning,3D Generation]
tags: [paper]     # TAG names should always be lowercase
author: <author_id> 
description: 一篇关于3d扩散模型的阅读笔记
---
# Autodecoding Latent 3D Diffusion Models

![image-20240831234043661](/assets/img/posts/2024-09-01-3d-diffusion-model.assets/image-20240831234043661.png)

## 1 Introduction

**背景**

> 照片真实感生成技术正在经历一个未来学者可能会将其与启蒙时代相提并论的时期。一年多前，人们还无法想象图像在质量、构图、风格化、分辨率、规模和操控能力方面的进步。丰富的在线图像，通常伴有文字、标签、标记，有时还有像素级分割，显著加速了这一进展。去噪扩散概率模型（DDPMs）的出现和发展推动了图像合成以及其它领域，例如音频([10, 20, 95])和视频([24, 89, 82, 29, 25, 47])的进步。
>
> 然而，世界是三维的，由静态和动态对象组成。它的几何和时间性质对生成方法提出了重大挑战。首先，我们拥有的数据主要由图像和单目视频组成。对于某些有限类别的对象，我们拥有与之对应的多视图图像或视频的3D网格，这些通常是通过繁琐的捕获过程获得的，或者是艺术家手动创建的。其次，与CNN不同，目前还没有广泛接受的适合3D几何和外观生成的3D或4D表示方法。因此，除了少数例外[74]，大多数现有的3D生成方法仅限于适合现有数据和常见几何表示的狭窄对象类别。移动的、有关节的对象，例如人类，使问题更加复杂，因为表示还必须支持变形。
> {: .prompt-info }

**我们的方法**

> 在这篇论文中，我们提出了一种新颖的方法来设计和训练适用于各种规模数据集的3D感知内容的去噪扩散模型。它通用到足以处理**刚性和有关节**的对象。它多功能到足以从静态和动态对象的多视图图像和单目视频中学习多样的3D几何和外观。在这些数据中识别对象的姿态已被证明对于学习有用的3D表示至关重要[6, 7, 73, 74]。因此，我们的方法被设计成对使用真实姿态、使用从运动中估计的姿态，或者根本不使用输入姿态信息，而是在训练过程中有效地学习它，都具有鲁棒性。它足够可扩展，能够在单一或多类别的数据集上进行训练，这些数据集包含大量多样化的对象，适合合成广泛的真实内容。
> {: .prompt-info }

**与以往工作对比**

> 最近的扩散方法包括两个阶段[63]。**在第一阶段，自动编码器学习一个丰富的潜在空间。为了生成新的样本，在第二阶段训练一个扩散过程来探索这个潜在空间。**为了训练一个图像到图像的自动编码器，需要大量的图像。同样，训练3D自动编码器需要大量的3D数据，而这些数据非常稀缺。以前的工作使用了如ShapeNet[8]这样的合成数据集（例如DiffRF[49]、SDFusion[12]等），因此仅限于这类数据可用的领域。
> {: .prompt-info }

![image-20240831151822653](/assets/img/posts/2024-09-01-3d-diffusion-model.assets/image-20240831151822653.png)

> 与这些工作不同，我们提议使用体积自动解码器来学习扩散采样的潜在空间。与基于自动编码器的方法不同，我们的自动解码器将训练集中的每个对象映射到一个1D向量，因此不需要3D监督。自动解码器从2D观测中学习3D表示，使用渲染一致性作为监督。遵循**UVA** [70]，这种3D表示支持模拟非刚性对象所需的有关节部分。
> {: .prompt-info }

**UVA**：用于非刚性可变形对象的无监督3D动画化

> 为了找到执行扩散的最佳中间表示，可以进行逐层的穷举搜索。然而，这样做在计算上非常昂贵，因为它需要运行数百个计算成本高昂的实验。相反，我们提出了可以应用于预训练且固定的自动解码器的任何层的**鲁棒归一化和去归一化操作**。这些操作计算鲁棒统计数据以执行层归一化，从而允许我们在自动解码器的任何中间分辨率上训练扩散过程。我们发现，在相当低的分辨率下，空间是紧凑的，为几何提供了必要的正则化，允许训练数据只包含每个对象的稀疏观测。另一方面，更深层次的层更多地作为上采样器运作。我们提供了广泛的分析，以找到适合我们基于自动解码器的扩散技术的适当分辨率。
> {: .prompt-info }

1. **“在相当低的分辨率下”**：指的是在自动解码器的不同层级中，选择一个相对较低的分辨率进行操作。分辨率通常指的是模型在空间上的细分程度，较低的分辨率意味着每个单位空间内包含的细节较少。
2. **“空间是紧凑的，为几何提供了必要的正则化”**：在低分辨率下，模型的参数数量较少，因此整个模型的“空间”（在这里指的是模型可以表示的所有可能的3D形状的集合）相对较小。正则化是机器学习中用来防止模型过拟合的技术。在这里，通过在较低分辨率下进行训练，模型被迫学习到更加泛化的特征，而不是记住训练数据中的每一个具体实例。这种泛化能力对于生成新的、未见过的数据（例如新的视角或姿态）时尤其重要。
3. **“允许训练数据只包含每个对象的稀疏观测”**：稀疏观测意味着对于每个对象，可用的训练数据相对较少。在这种情况下，如果模型过于复杂（例如在非常高的分辨率下训练），它可能会因为数据不足而无法正确学习对象的几何形状。但在低分辨率下，由于模型空间的紧凑性和正则化效果，即使只有少量的训练数据，模型也能够学习到如何生成合理的3D对象。

**实验**

> 我们展示了我们方法在涉及刚性与有关节3D对象合成的各种任务上的多功能性和可扩展性。我们首先在与DiffRF [49] 类似的设置中使用多视图图像和相机训练我们的模型，以生成有限数量对象类别的形状。然后，我们将模型扩展到使用真实世界的MVImgNet [92] 数据集训练的数十万种多样化对象，这超出了以前3D扩散方法的能力。最后，我们在包含大约44K序列的高质量人类运动视频的CelebV-Text [90] 的子集上训练我们的模型。
> {: .prompt-info }

## 2 Related Work

### 2.1 Neural Rendering for 3D Generation

> 神经辐射场，或称为NeRF（Mildenhall等人，2020年[48]），能够从2D图像中学习并实现高质量的新视角合成（NVS）对于刚性场景。它对体积神经渲染的方法已成功应用于各种任务，包括生成适合3D感知NVS的对象。受到生成对抗模型（GANs）[22]在生成2D图像[22, 5, 32, 33, 35, 34]和视频[79, 72, 91]方面的快速发展的启发，后续的工作将它们扩展到使用神经渲染技术的3D内容生成。这样的工作[67, 51, 54, 52, 88]在这项任务上展示了有希望的结果，但受限于从任意视点的多视图一致性有限，并且在泛化到多类别图像数据集方面遇到了困难。
>
> 在这个领域的一项值得注意的工作是pi-GAN（Chan等人，2021年[6]），它采用具有周期性激活函数的神经渲染，以实现视角一致的渲染生成。然而，它需要对数据集相机姿态分布进行精确估计，这限制了它对自由视点视频的适用性。在随后的工作中，EG3D（Chan等人，2022年[7]）和EpiGRAF（Skorokhodov等人[73]）使用基于StyleGAN2（Karras等人，2020年[35]）的生成器-鉴别器框架创建的3D场景的三平面表示。**然而，这些工作需要从关键点（例如面部特征）进行姿态估计来进行训练，这再次限制了视点范围。**
>
> 这些工作主要在有限的形状和外观变化内生成单一对象类别的内容。一个值得注意的例外是3DGP [74]，它能够泛化到ImageNet [15]。**然而，它依赖于单目深度预测，这限制了它只能生成面向前方的场景。**这些限制也阻止了这些方法处理可变形的、有关节的对象。相比之下，我们的方法适用于可变形和刚性对象，并涵盖了更广泛的视野范围。
> {: .prompt-info }

1. **视点范围限制**：依赖于关键点的姿态估计通常意味着模型只能从特定的角度或视点生成准确的3D内容。这是因为关键点信息通常只涵盖了有限的视角，并且可能无法全面捕捉对象在所有可能视角下的形态变化。因此，当尝试从模型未见过的新视角生成内容时，可能会遇到质量问题，如失真或不一致。
2. **缺乏尺度信息**：单目相机由于只有一个视角，无法直接提供场景的绝对尺度信息。这意味着预测的深度值通常需要一个先验的尺度假设或通过其他方式进行尺度校正。

### 2.2 Denoising Diffusion Modeling

> 去噪扩散概率模型（DDPMs）[75, 28] 将生成过程表示为通过一系列扩散步骤逐渐被污染的数据的学习去噪。随后的工作在改进训练目标、架构和采样过程方面[28, 17, 86, 36, 63, 53, 76]展示了在各个数据领域生成高质量数据的快速进展。然而，这些工作主要展示了目标领域样本完全可观察的任务的结果，而不是在仅对数据集内容进行部分观察的领域中操作。
>
> 这样的领域中最重要的之一是3D数据，对于大多数现实世界的内容来说，这些数据主要在2D图像中观察到。近期的一些工作已经在这方面展示了有希望的初步结果。DiffRF [49] 提出了对合成数据集的每个对象重建NeRF体积，然后在U-Net框架内对它们进行扩散训练。然而，它需要重建许多对象体积，并且由于扩散训练的高计算成本，仅限于低分辨率体积。相比之下，我们的框架在自动解码器的潜在空间中运作，有效地共享了所有训练数据中学到的知识，从而实现了低分辨率的潜在3D扩散。在文献[12]中，使用3D自动编码器生成3D形状，但这种方法需要真实的3D监督，并且只专注于形状生成，纹理添加则使用现成的方法[60]。相反，我们的框架学会了在没有这种真实的3D监督的情况下生成表面外观和相应的几何形状。
>
> 许多最近的研究[2, 68, 23, 9]结合了去噪扩散方法和三平面表示[7]来进行3D生成。它们在自动解码器[2]的嵌入向量上、自动编码器[23]的瓶颈处，或者直接在预先计算的[68]或同时学习的三平面[9]上执行扩散。然而，这些研究集中在小数据集上，或者需要密集的点云和真实的对象网格，这些对于真实对象的图像数据集来说并不容易获得。三平面表示需要一个MLP解码器，这显著增加了体积渲染的时间。我们的体素解码器没有这样的要求，因为它直接输出颜色和密度，从而允许在大规模真实图像数据集上进行更快的训练。
>
> 最近，有几项研究[60, 43, 11]提议使用大规模预训练的文本到图像的2D扩散模型来进行3D生成。这些方法背后的关键是使用2D扩散模型来评估从随机采样的视点渲染的质量，然后利用这些信息来优化内容的3D感知表示。然而，与我们的方法相比，这些方法需要一个成本更高的优化过程来生成每一个新的对象。
> {: .prompt-info }

#### Note

##### 马尔可夫链

[简述马尔可夫链【通俗易懂】 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/448575579)

##### 扩散模型 (Diffusion Model) 

[扩散模型 - Diffusion Model【李宏毅2023】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV14c411J7f2/?spm_id_from=..search-card.all.click&vd_source=9bfbfed996a95c99c94d2392e74aa10b)

对于一个从高斯分布随机采样的噪点图片，我们希望**逐步去噪**，生成一张全新的图片。

![image-20240901155008936](/assets/img/posts/2024-09-01-3d-diffusion-model.assets/image-20240901155008936.png)

怎么样**逐步去噪**？

答：利用一个**噪声预测器**，并用输入图片减去噪声。

![image-20240901155113663](/assets/img/posts/2024-09-01-3d-diffusion-model.assets/image-20240901155113663.png)

怎么训练**噪声预测器**？

![image-20240901155344524](/assets/img/posts/2024-09-01-3d-diffusion-model.assets/image-20240901155344524.png)

答：对训练集的图像不断加噪声，直到近似于从高斯分布随机采样的图像。那么每一步添加的噪声就是真实数据。

怎么样实现**文生图**？

![image-20240901155738371](/assets/img/posts/2024-09-01-3d-diffusion-model.assets/image-20240901155738371.png)

答：将文字描述作为输入之一即可。

## 3 Methodology

**两阶段方法**

> 在第一阶段，我们学习一个自动解码器G，它包含一个与训练数据集中的对象相对应的嵌入向量库。这些向量首先被处理以创建一个低分辨率的潜在3D特征体积，然后逐渐上采样，最终解码成生成对象的形状和外观的体素表示。这个网络是使用体渲染技术在该体积上训练的，并利用训练图像的2D重建监督。
> {: .prompt-info }

#### Note

<img src="/assets/img/posts/2024-09-01-3d-diffusion-model.assets/image-20240831233632784.png" alt="image-20240831233632784" style="zoom: 33%;" />

##### 输入

自动解码器将训练集中的每个3D对象表示为一个一维（1D）向量

##### **G1 - 潜在体积生成器（Latent Volume Generator）**

- G1的作用是将输入的1D向量（通常是通过哈希函数从对象索引生成的）处理成一个低分辨率的潜在3D特征体积。
- G1逐步将1D向量上采样到多个分辨率的潜在特征，直到最终生成一个适合进行进一步处理的体积表示。

**潜在3D特征体积**通常是一个三维的网格（或称为体素（voxel）网格），每个网格单元（体素）包含一组特征。这些特征可以是多维的，例如在某些模型中，每个体素可能包含颜色、密度、法线或其他与材质和光照相关的属性。

假设一个简化的例子，一个潜在3D特征体积可能是一个32×32×32的体素网格，每个体素包含3个通道的数据（例如RGB颜色值）和一个密度值。这个体积整体上描述了一个3D空间内每个位置的颜色和存在概率，从而可以用于生成和渲染3D对象。

##### **G2 - 解码器（Decoder）：**

- G2的任务是将G1生成的低分辨率潜在体积进一步上采样和细化，最终解码成更高分辨率的体积表示，这包括了对象的辐射度（radiance）和密度（density）体积。
- 这些体积表示可以用于渲染，以生成从新视角观察时保持外观和几何一致性的2D图像。
- G2通常包含一系列的3D转置卷积层（也称为反卷积层），用于将潜在特征体积上采样到所需的输出分辨率。

> 在第二阶段中，我们将自解码器 $G$ 拆分为两部分，$G = G2 \circ G1$。然后，我们利用这个自解码器来训练一个在从 $G1$ 获得的紧凑的三维潜在空间中运行的三维扩散模型。
> {: .prompt-info }

#### Note

<img src="/assets/img/posts/2024-09-01-3d-diffusion-model.assets/image-20240901163907718.png" alt="image-20240901163907718" style="zoom: 33%;" />



> 通过从自解码器训练数据集中提取的结构和外观属性，这个三维扩散过程使我们能够使用该网络高效地生成多样且逼真的三维内容。完整的流程如图1所示。
> {: .prompt-info }

<img src="/assets/img/posts/2024-09-01-3d-diffusion-model.assets/image-20240901164114546.png" alt="image-20240901164114546" style="zoom: 50%;" />

**完整流程：**

![image-20240901164046256](/assets/img/posts/2024-09-01-3d-diffusion-model.assets/image-20240901164046256.png)

> 首先，我们描述体积自解码架构（第 3.1 节）。接着，我们介绍自解码器的训练过程和重建损失（第 3.2 节）。最后，我们详细说明在解码器潜在空间中进行 3D 扩散的训练和采样策略（第 3.3 节）。
> {: .prompt-info }

### 3.1 Autodecoder architecture

**规范表示**

> 我们使用三维体素网格来表示物体的三维结构和外观。我们假设物体处于其规范姿态，这样三维表示就与相机姿态解耦。此解耦对于学习紧凑的物体表示是必要的，同时也作为一种必要的约束，以便在没有直接三维监督的情况下，从二维图像中学习有意义的三维结构。具体来说，规范体素表示由一个密度网格 $V_{\text{Density}} \in \mathbb{R}^{S^3}$ 组成，它是一个分辨率为 $S^3$ 的密度场的离散表示，以及一个表示 RGB 辐射场的 $V_{\text{RGB}} \in \mathbb{R}^{S^3 \times 3}$。我们采用体积渲染的方法，将辐射和不透明度值沿每条视线进行积分，类似于 NeRFs [48]。然而，与原始 NeRF 不同的是，我们不是使用多层感知机（MLP）来计算这些局部值，而是从解码的体素网格中三线性插值密度和 RGB 值，这与 Plenoxels [64] 类似。
> {: .prompt-info }

**体素解码器**

> 密度和辐射的三维体素网格 $V_{\text{Density}}$ 和 $V_{\text{RGB}}$ 是通过体积自解码器 $G$ 生成的，该解码器使用二维图像的渲染监督进行训练。我们选择直接生成 $V_{\text{Density}}$ 和 $V_{\text{RGB}}$，而不是使用特征体积或三平面等中间表示，因为直接生成更有利于渲染，并确保跨多个视图的一致性。需要注意的是，特征体积和三平面表示需要对每个采样点运行多层感知机（MLP）传递，这在训练和推理过程中会带来显著的计算成本和内存开销。
> {: .prompt-info }

**解码器的学习方式**

> **解码器的学习方式**与 GLO [4] 类似，适用于来自大规模多视图或单目视频数据集的各种物体类别。我们的自解码器架构改编自 [70]。然而，在我们的框架中，我们希望支持大规模数据集，这在设计能够跨多个类别生成高质量三维内容的解码器架构方面提出了挑战。为了表示我们最大数据集中大约 30 万个物体，我们需要一个容量非常高的解码器。由于我们发现 [70] 中相对基础的解码器产生了较差的重建质量，我们引入了以下关键扩展：
>
> - 为了支持目标数据集中多样化的形状和外观，我们发现将解码器学习到的嵌入向量长度从 64 增加到 1024 是至关重要的。
>
>
> - 我们将自解码器中每个分辨率下的残差块数量从 1 增加到 4。
> - 最后，为了统一重建物体的外观，我们在第二和第三级（分辨率为 $8^3$ 和 $16^3$）引入了自注意力层 [81]。
>   {: .prompt-info }

**对大型数据集进行嵌入码本的扩展**

> 训练集中每个对象都被编码为一个嵌入向量。然而，尤其是对于大型数据集，为每个对象存储一个单独的向量非常繁琐。因此，我们提出了一种技术，可以显著减少嵌入参数的占用，同时仍然允许从大规模数据集中进行有效生成。类似于StyleGenes的方法[55]，我们通过组合更小的嵌入子向量来创建唯一的每对象向量。解码器的输入是每个对象的嵌入向量 $H_k \in \mathbb{R}^l$，长度为 $l_v$。它是更小的子向量 $h^j_i$ 的拼接，其中每个子向量是从一个有序的码本中选择的，码本有 $n_c$ 个条目，每个条目包含 $n_h$ 个长度为 $l_v/n_c$ 的嵌入向量： 
>
> $$
> H_k = [h_{k1}^1, h_{k2}^2, \ldots, h_{knc}^{nc}]
> $$
> 其中 $k_i \in \{1, 2, \ldots, n_h\}$ 是用于从 $n_h$ 个可能的码本条目中选择位置 $i$ 上的最终向量的索引集合。此方法允许嵌入向量的组合数呈指数增长，与单个嵌入码本相比，显著减少了存储的参数数量。与[55]相反，位置 $i$ 处向量 $h^j_i$ 的索引 $j$ 不是随机选择的，而是使用一个哈希函数[16]将每个训练对象的索引 $k$ 映射到其对应的嵌入索引。
> {: .prompt-info }

### 3.2 Autodecoder Training

> 我们通过分析合成从图像数据中训练解码器，主要目标是最小化解码器生成的图像与训练图像之间的差异。我们使用体积渲染[48]生成RGB彩色图像 $C$，此外，为了监督对象的轮廓，我们还生成二维占用掩码 $O$。
> {: .prompt-info }

**金字塔感知损失**

> 与文献[69, 70]中的方法类似，我们基于[31]在渲染图像上采用金字塔感知损失作为主要重建损失：
>
> $$
> L_{\text{rec}}(\hat{C}, C) = \sum_{l=0}^{L} \sum_{i=0}^{I} \left| \text{VGG}_i(D_l(\hat{C})) - \text{VGG}_i(D_l(C)) \right|,
> $$
> 其中，$\hat{C}, C \in [0, 1]^{H \times W \times 3}$ 分别表示分辨率为 $H \times W$ 的RGB渲染图像和训练图像；$\text{VGG}_i$ 是预训练的VGG-19[71]网络的第 $i$ 层；运算符 $D_l$ 将图像下采样到金字塔层级 $l$ 的分辨率。
> {: .prompt-info }

**前景监督**

> 由于我们只关注单个对象的建模，在本文所考虑的所有数据集中，我们都移除了背景。然而，如果对象的颜色是黑色（这对应于密度的缺失），网络可能会使对象变得半透明。为了改善重建对象的整体形状，我们使用前景监督损失。使用二值前景掩码（根据数据集的不同，通过现成的抠图方法[44]、Segment Anything[39]或合成的真实掩码估计），我们在渲染的占用图上应用L1损失，以匹配与图像对应的掩码：
>
> $$
> L_{\text{seg}}(\hat{O}, O) = \frac{1}{HW} \|O - \hat{O}\|_1,
> $$
> 其中，$\hat{O}, O \in [0, 1]^{H \times W}$ 分别表示推断的和真实的占用掩码。我们在补充材料中提供了此损失的推断几何形状的视觉比较。
> {: .prompt-info }

**多帧训练**

> 由于我们的新解码器具有较大的容量，生成一个体积与基于该体积渲染图像相比会产生更大的开销（主要是体素立方体的三线性采样）。因此，在每个批次中，我们不是为目标对象的规范表示渲染一个视图，而是为批次中的每个对象渲染四个视图。这种技术不会带来显著的额外开销，并且有效地将批次大小增加了四倍。一个额外的好处是，我们发现这种技术可以提高生成结果的整体质量，因为它显著减少了批次方差。我们对这种技术以及我们的关键架构设计选择进行了消融实验，展示了它们对样本质量的影响（第4.3节，表2）。
> {: .prompt-info }

**学习非刚性物体**

> 对于有关节的非刚性物体，例如人类主体的视频，我们必须从动态姿态中建模主体的形状和局部运动，以及相应的局部区域的非刚性变形。按照[70]的方法，我们假设这些序列可以分解为一组 $N_p$ 个较小的刚性组件（在我们的实验中为10个），其姿态可以在规范的三维空间中进行一致的对齐估计。每个组件的相机姿态在训练过程中被估计并逐步优化，使用的是每个被描绘主体组件的学习3D关键点及其在每张图像中预测的对应2D投影。这种估计是通过一个可微分的PnP（Perspective-n-Point）算法[40]来实现的。为了将这些组件与合理的变形相结合，我们采用了一种学习的体积线性混合蒙皮（LBS）操作。我们引入了一个体素网格 $V^{\text{LBS}} \in \mathbb{R}^{S^3 \times N_p}$ 来表示每个变形组件的蒙皮权重。由于我们假设对对象组件的内容或分配没有先验知识，因此每个组件的蒙皮权重也在训练过程中进行估计。更多细节请参见补充材料。
> {: .prompt-info }

### 3.3 Latent 3D Diffusion

**架构**

> 我们的扩散模型架构将先前在二维空间中进行扩散的工作[36]扩展到潜在的三维空间。我们在三维解码器空间中实现其二维操作，包括卷积和自注意力层。在文本条件实验中，在自注意力层之后，**我们使用类似于[63]的交叉注意力层。更多细节请参见补充材料。**
> {: .prompt-info }

**特征处理**

> 我们的一项关键观察是，三维自动解码器的潜在空间中的特征 $F$ 具有钟形分布（见补充材料），这使得不需要对其强制执行任何形式的先验，例如[63]中的方法。在没有先验的潜在空间中操作，使得能够为每种可能的潜在扩散分辨率训练一个单一的自动解码器。然而，我们观察到特征分布 $F$ 有非常长的尾部。我们推测这是因为网络推断的最终密度值没有任何自然的界限，因此可以落在任意范围内。事实上，网络被鼓励做出这样的预测，因为它们在表面和空白区域之间具有最清晰的边界。
>
> 然而，为了在所有数据集和所有训练的自动解码器上使用统一的一组扩散超参数，我们必须将它们的特征标准化到相同的范围。这相当于计算分布的中心和尺度。请注意，由于特征分布的长尾特性，典型的均值和标准差统计量将受到严重偏差。因此，我们提出了一种基于特征分布分位数的稳健替代方法。我们取中位数 $m$ 作为分布的中心，并使用正态分布的归一化四分位距（IQR）[85] 0.7413 × IQR 来近似其尺度。在将特征 $F$ 用于扩散之前，我们将它们标准化为 $\hat{F} = \frac{F - m}{\text{IQR}}$。在推理过程中，当生成最终体积时，我们将其反标准化为 $\hat{F} \times \text{IQR} + m$。我们称这种方法为稳健标准化。其影响的评估请参见补充材料。
> {: .prompt-info }

**对象生成的采样**

> 在推理过程中，我们依赖于来自EDM [36]的采样方法，并进行了若干小的修改。我们将EDM的超参数固定为匹配数据集的分布，无论实验如何，都设为0.5，并在我们的特征处理步骤中修改了特征统计量。我们还在文本条件实验中引入了无分类器引导[27]（第4.5节）。我们发现将权重设置为3，在所有数据集中都能取得良好的效果。
> {: .prompt-info }